{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using state ParaÃ­ba server backend.\n",
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from math import ceil\n",
    "\n",
    "import translators.server as ts\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "import nltk\n",
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36', 'Accept-Language':'pt-BR,pt;q=0.9,en;q=0.8'}\n",
    "s = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier_sentiment = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "Running on public URL: https://d66736e4dcf97a97.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d66736e4dcf97a97.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "        \n",
    "    input = gr.Textbox(label=\"Nome do produto\")\n",
    "\n",
    "    product = \"\"\n",
    "    btn_pesq = gr.Button(\"Pesquisar\")\n",
    "    \n",
    "    output = gr.outputs.Textbox(label=\"console\")\n",
    "    label_out = gr.outputs.Label(label=\"sentiment\")\n",
    "    label_out2 = gr.outputs.Label(label=\"frequency\", num_top_classes=10)\n",
    "\n",
    "    state = gr.State(value=product)\n",
    "\n",
    "    array_dict = []\n",
    "\n",
    "    def produto(input, stats):\n",
    "        reviews = []\n",
    "        produtos = []\n",
    "        product = \"\"\n",
    "        stats = ''\n",
    "        text = str(input) + ' amazon'\n",
    "        url = \"https://www.google.com/search?q=\" + text\n",
    "\n",
    "        request_result=s.get(url).content\n",
    "\n",
    "        soup = BeautifulSoup(request_result,\"html.parser\")\n",
    "\n",
    "\n",
    "        for a in soup.find_all('a', href=True):    \n",
    "            if 'amazon.com' in a['href'] and '/dp/' in a['href']:\n",
    "                href888 = a['href']\n",
    "                produtos.append(a.text)\n",
    "                new_url = href888.replace('/url?q=', '') # remove /url?q=\n",
    "                url_coments = new_url.split('&')[0].replace('/dp/', '/product-reviews/').replace('.br', '')\n",
    "                if '//'  in url_coments:\n",
    "                    url_coments = url_coments.split('//')[1]\n",
    "                reviews.append('https://' + url_coments)\n",
    "\n",
    "        coments = []\n",
    "        cont=0\n",
    "\n",
    "        for r in reviews:\n",
    "            \n",
    "            try:\n",
    "                if len(coments) >= 1000:\n",
    "                    break\n",
    "                html = s.get(r + f'/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber=1', headers=headers).content\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "                a_soupi = soup.find_all('div', class_='a-row a-spacing-base a-size-base')\n",
    "                \n",
    "                if len(a_soupi) == 0: # se não encontrar a div, pula para o próximo produto\n",
    "                    continue\n",
    "                \n",
    "                stats += f\"Analisando produto {produtos[cont]}\\n\"\n",
    "                stats += a_soupi[0].text.replace('\\n', '').replace('  ', '') + '\\n' + '\\n'\n",
    "                yield {}, {}, stats, stats\n",
    "                \n",
    "                cont+=1 \n",
    "                \n",
    "                i = int(a_soupi[0].text.replace('\\n', '').replace('  ', '').replace(',', '').split(' ')[3])/10 # pegando o numero de páginas\n",
    "                i = ceil(i)\n",
    "\n",
    "\n",
    "                if i == 1: # se só tiver uma página roda o for só uma vez\n",
    "                    i+=1\n",
    "                \n",
    "\n",
    "                for j in range(1, i):\n",
    "                    if len(coments) >= 1000:\n",
    "                        break    \n",
    "                    html = s.get(r + f'/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber={j}', headers=headers).content\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    a_soup = soup.find_all('div', class_='a-row a-spacing-small review-data')\n",
    "                    \n",
    "                    for a in a_soup:\n",
    "                        for coment in a.text.split('\\n'):\n",
    "                            # limite de comentários, para o programa não ficar muito lento\n",
    "                            if len(coments) >= 1000:\n",
    "                                break\n",
    "                            if coment != '':\n",
    "                                coments.append(coment)\n",
    "                \n",
    "            \n",
    "            except Exception as e:\n",
    "                stats += e\n",
    "                yield {}, {}, stats, stats\n",
    "                continue\n",
    "\n",
    "             \n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        coments_translate = []\n",
    "        cont1 = 1\n",
    "\n",
    "        # pelo fato de ter comentários de diferentes linguas, só iremos pegar os comentários em inglês\n",
    "\n",
    "        for c in coments:\n",
    "            stats=''\n",
    "            stats+=f'Detectando comentários em inglês\\n'\n",
    "            yield {}, {}, stats, stats\n",
    "            try:\n",
    "                det = detect(c)\n",
    "                if det != 'en' or 'The media could not be loaded.' in c:\n",
    "                    continue\n",
    "                else:\n",
    "                    coments_translate.append(c)\n",
    "                    cont1+=1\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            stats+=f'{cont1} de {len(coments)} comentários\\n'\n",
    "            yield {}, {}, stats, stats\n",
    "        \n",
    "        stats=''\n",
    "        stats+=f'{len(coments_translate)} comentários válidos\\n'\n",
    "        yield {}, {}, stats, stats\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "        \n",
    "        ## Analisando modelo\n",
    "        \n",
    "\n",
    "        nltk.download('stopwords')\n",
    "        stopwords_en = stopwords.words(\"english\")\n",
    "\n",
    "        spacy.cli.download(\"en_core_web_sm\")\n",
    "        spc_en = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "        def limpa_texto(texto):\n",
    "            '''(str) -> str\n",
    "            Essa funcao recebe uma string, deixa tudo em minusculo, filtra apenas letras,\n",
    "            retira stopwords, lemmatiza e retorna a string resultante.\n",
    "            '''\n",
    "            texto = texto.lower()\n",
    "\n",
    "            texto = re.sub(r\"[\\W\\d_]+\", \" \", texto)\n",
    "\n",
    "            texto = [pal for pal in texto.split() if pal not in stopwords_en]\n",
    "\n",
    "            spc_texto = spc_en(\" \".join(texto))\n",
    "            tokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in spc_texto]\n",
    "            \n",
    "            return \" \".join(tokens)\n",
    "\n",
    "        try:\n",
    "            df_alexa = pd.read_csv('df_preprocessed.csv')\n",
    "            df_alexa.dropna(inplace=True)\n",
    "    \n",
    "        except:\n",
    "            stats=''\n",
    "            stats+='Dataset preprocessado nao encontrado, criando novo dataset...\\n'\n",
    "            yield {}, {}, stats, stats\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')\n",
    "\n",
    "                df_alexa = df.drop(['id','dateAdded', 'dateUpdated', 'asins', 'keys', 'manufacturer', 'manufacturerNumber', \n",
    "                'reviews.date', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id',\n",
    "                'reviews.numHelpful', 'reviews.sourceURLs', 'reviews.username', 'imageURLs', 'primaryCategories', 'categories',\n",
    "                'brand', 'name', 'sourceURLs'], axis=1)\n",
    "\n",
    "                df_alexa.fillna('', inplace = True) # para nao ter problemas com nulos na concatenacao\n",
    "\n",
    "                # concatenando as duas colunas\n",
    "                df_alexa['verified_reviews'] = df_alexa['reviews.text'] + ' ' + df_alexa['reviews.title']\n",
    "                # removendo entradas sem texto\n",
    "                df_alexa = df_alexa[df_alexa['verified_reviews'] != ' ']\n",
    "\n",
    "\n",
    "                # transformando rating em feedback 0 e 1\n",
    "                labels = []\n",
    "                for score in df_alexa['reviews.rating']:\n",
    "                    if score > 3:\n",
    "                        labels.append(1)\n",
    "                    else:\n",
    "                        labels.append(0)\n",
    "\n",
    "                df_alexa['feedback'] = labels\n",
    "                # Aplica a funcao nas reviews do dataset\n",
    "                df_alexa['verified_reviews'] = df_alexa['verified_reviews'].apply(limpa_texto)\n",
    "\n",
    "                # Salva o dataset preprocessado\n",
    "                \n",
    "                df_alexa.to_csv('df_preprocessed.csv', index=False)\n",
    "            except Exception as e:\n",
    "                \n",
    "                stats=''\n",
    "                stats+=e + '\\n'\n",
    "                yield {}, {}, stats, stats\n",
    "                exit()\n",
    "            \n",
    "\n",
    "\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        texto = df_alexa['verified_reviews']\n",
    "        # Importando o TfidfVectorizer\n",
    "\n",
    "        # Instanciando o TfidfVectorizer\n",
    "        tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "        # Vetorizando\n",
    "        X_tfidf = tfidf_vect.fit_transform(texto)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_tfidf.toarray(), df_alexa['feedback'], test_size = 0.2)\n",
    "\n",
    "\n",
    "        try:\n",
    "            # load json and create model\n",
    "            json_file = open('model.json', 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            classifier = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            classifier.load_weights(\"model.h5\")\n",
    "            stats=''\n",
    "            stats+='Modelo carregado com sucesso\\n'\n",
    "            yield {}, {}, stats, stats\n",
    "\n",
    "            # evaluate loaded model on test data\n",
    "            classifier.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "        except:\n",
    "            # create model and save\n",
    "            classifier = tf.keras.models.Sequential()\n",
    "            classifier.add(tf.keras.layers.Dense(units = 10, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "            classifier.add(tf.keras.layers.Dense(units = 10, activation='relu'))\n",
    "            classifier.add(tf.keras.layers.Dense(units = 1, activation='sigmoid'))\n",
    "\n",
    "            classifier.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "            # fit model\n",
    "            epochs_hist = classifier.fit(X_train, y_train, epochs=100, batch_size=50,  verbose=2, validation_split=0.2)\n",
    "            stats=''\n",
    "            stats+='Modelo criado com sucesso\\n'\n",
    "            yield {}, {}, stats, stats\n",
    "\n",
    "\n",
    "            # serialize model to JSON\n",
    "            model_json = classifier.to_json()\n",
    "            with open(\"model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            classifier.save_weights(\"model.h5\")\n",
    "            stats+='Salvo modelo em disco\\n'\n",
    "            yield {}, {}, stats, stats\n",
    "\n",
    "\n",
    "        if len(coments_translate) == 0:\n",
    "            stats=''\n",
    "            stats+='Nenhum comentário para ser classificado\\n'\n",
    "            yield {}, {}, stats, stats\n",
    "        else:\n",
    "            text = []\n",
    "            for t in coments_translate:\n",
    "                texto = limpa_texto(t)\n",
    "                    \n",
    "                text.append(texto)\n",
    "            # c = tfidf_vect.transform(coments_translate).toarray()\n",
    "            c = tfidf_vect.transform(text).toarray()\n",
    "            predict = (classifier.predict(c) > 0.5).astype(int)\n",
    "\n",
    "            cont_pos = 0\n",
    "            cont_neg = 0\n",
    "            for p in predict:\n",
    "                if p == 1:\n",
    "                    cont_pos += 1\n",
    "                else:\n",
    "                    cont_neg += 1\n",
    "\n",
    "            stats=''\n",
    "            stats+=f'Positivos: {cont_pos} | Negativos: {cont_neg}\\n'\n",
    "            yield {}, {}, stats, stats\n",
    "\n",
    "            indexes_neg = np.where(predict == 0)[0] # obtendo indexes dos comentarios negativos\n",
    "\n",
    "            if len(indexes_neg) == 0:\n",
    "                stats+='Nenhum comentario negativo encontrado\\n'\n",
    "                yield {}, {}, stats, stats\n",
    "\n",
    "            else:\n",
    "                stats+='Comentarios negativos traduzidos para portugues:\\n\\n'\n",
    "                yield {}, {}, stats, stats\n",
    "                \n",
    "                # text = []\n",
    "                # for t in coments_translate:\n",
    "                #     texto = limpa_texto(t)\n",
    "                        \n",
    "                #     text.append(texto)\n",
    "\n",
    "                for i in indexes_neg:\n",
    "\n",
    "                    comentario_class = classifier_sentiment(text[i])[0]\n",
    "\n",
    "                    new_dict = {}\n",
    "                    for d in comentario_class:\n",
    "                        new_dict[d['label']] = d['score']\n",
    "                    \n",
    "                    array_dict.append(new_dict)\n",
    "                    \n",
    "                    # somando os scores de cada label\n",
    "                    dict_soma = {}\n",
    "                    t = len(array_dict)\n",
    "\n",
    "                    for a in array_dict:\n",
    "                        for key, value in a.items():\n",
    "                            # print(key, value)\n",
    "                            if key not in dict_soma:\n",
    "                                dict_soma[key] = value\n",
    "                            \n",
    "                            else:\n",
    "                                dict_soma[key] += value\n",
    "\n",
    "                    for key, value in dict_soma.items():\n",
    "                        dict_soma[key] = value/t\n",
    "                    \n",
    "\n",
    "\n",
    "                    # frequencia de palavras\n",
    "                    \n",
    "                    word2count = {}\n",
    "                    for data in text:\n",
    "                        words = nltk.word_tokenize(data)\n",
    "                        for word in words:\n",
    "                            if word not in word2count.keys():\n",
    "                                word2count[word] = 1\n",
    "                            else:\n",
    "                                word2count[word] += 1\n",
    "                    \n",
    "                    soma_freq =  (sum(word2count.values()))\n",
    "                    for keys, values in word2count.items():\n",
    "                        word2count[keys] = values/soma_freq\n",
    "                    \n",
    "                    stats+=ts.google(coments_translate[i], to_language='pt') + '\\n\\n'\n",
    "                    yield word2count, dict_soma, stats, stats\n",
    "\n",
    "        \n",
    "    \n",
    "    def sentiment(input):\n",
    "      t = len(input)\n",
    "\n",
    "      new_dict = {}\n",
    "      if t !=0:\n",
    "\n",
    "        soma_score = 0\n",
    "        for d in input[t-1]:\n",
    "          soma_score += d['score']\n",
    "          new_dict[d['label']] = soma_score/t\n",
    "      \n",
    "      return new_dict\n",
    "\n",
    "    btn_pesq.click(produto, [input, state], [label_out2, label_out, state, output])   \n",
    "\n",
    "demo.queue()\n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_soma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
