{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using state ParaÃ­ba server backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import numpy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from math import ceil\n",
    "\n",
    "import translators.server as ts\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36', 'Accept-Language':'pt-BR,pt;q=0.9,en;q=0.8'}\n",
    "s = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'Tabu (Dana) - Feminino'\n",
    "text = input + ' amazon'\n",
    "url = \"https://www.google.com/search?q=\" + text\n",
    "\n",
    "request_result=s.get(url).content\n",
    "\n",
    "soup = BeautifulSoup(request_result,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "\n",
    "new_url = ''\n",
    "reviews = []\n",
    "produtos = []\n",
    "for a in soup.find_all('a', href=True):    \n",
    "    if 'www.amazon.com' in a['href'] and '/dp/' in a['href']:\n",
    "        href888 = a['href']\n",
    "        produtos.append(a.text)\n",
    "        new_url = href888.replace('/url?q=', '') # remove /url?q=\n",
    "        url_coments = new_url.split('&')[0].replace('/dp/', '/product-reviews/').replace('.br', '')\n",
    "        if '//'  in url_coments:\n",
    "            url_coments = url_coments.split('//')[1]\n",
    "        reviews.append('https://' + url_coments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisando produto TABU by Dana - Cologne Spray 70 ml for Women - Amazon.comwww.amazon.com › TABU-Dana-Cologne-Spray-W...\n",
      "34 total ratings, 2 with reviews\n",
      "\n",
      "Analisando produto Amazon.com : Tabu by Dana For Women EDC 3.0 OZ SPRwww.amazon.com › Tabu-Dana-Women-EDC-3-0\n",
      "217 total ratings, 24 with reviews\n",
      "\n",
      "Analisando produto TABU ROSE fragrâncias finas MIST 8,0 FL. OZ. Por Dana CLASSIC ...www.amazon.com.br › TABU-fragrâncias-finas-CLASSIC-FRAGRANCES\n",
      "81 total ratings, 16 with reviews\n",
      "\n",
      "Analisando produto Deo Colônia Segredos, Tabu, 60 Ml - Amazonwww.amazon.com.br › Tabu-Segredos-Deo-Colônia-60ml\n",
      "280 total ratings, 69 with reviews\n",
      "\n",
      "Analisando produto Deo Colônia Tabu Flores 60 Ml, | Amazon.com.brwww.amazon.com.br › Colônia-Feminina-60Ml-Flores-Unit\n",
      "14 total ratings, 1 with review\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coments = []\n",
    "cont=0\n",
    "for r in reviews:\n",
    "    \n",
    "    try:\n",
    "        html = s.get(r + f'/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber=1', headers=headers).content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        a_soupi = soup.find_all('div', class_='a-row a-spacing-base a-size-base')\n",
    "        \n",
    "        if len(a_soupi) == 0:\n",
    "            continue\n",
    "        print(f\"Analisando produto {produtos[cont]}\")\n",
    "        print(a_soupi[0].text.replace('\\n', '').replace('  ', ''))\n",
    "        i = int(a_soupi[0].text.replace('\\n', '').replace('  ', '').replace(',', '').split('ratings ')[1].split(' ')[0])/10\n",
    "        i = ceil(i)\n",
    "\n",
    "        for j in range(1, i):\n",
    "            html = s.get(r + f'/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber={j}', headers=headers).content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            a_soup = soup.find_all('div', class_='a-row a-spacing-small review-data')\n",
    "            \n",
    "            for a in a_soup:\n",
    "                for coment in a.text.split('\\n'):\n",
    "                    if coment != '':\n",
    "                        coments.append(coment)\n",
    "        \n",
    "    except:\n",
    "        print('error')\n",
    "        continue\n",
    "\n",
    "    cont+=1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduzindo 90 comentários...)\n"
     ]
    }
   ],
   "source": [
    "coments_translate = []\n",
    "print(f\"Traduzindo {len(coments)} comentários...)\")\n",
    "for c in coments:\n",
    "    try:\n",
    "        detec = detect(c)\n",
    "        if detec != 'en':\n",
    "            len(c)\n",
    "            coments_translate.append(ts.google(c, to_language='en'))\n",
    "        else:\n",
    "            coments_translate.append(c)\n",
    "    except:\n",
    "        try:\n",
    "            coments_translate.append(ts.google(c, to_language='en'))\n",
    "        except:\n",
    "            \n",
    "            continue\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 de 90 comentário traduzidos válidos\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(coments_translate)} de {len(coments)} comentário traduzidos válidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_en = stopwords.words(\"english\")\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spc_en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpa_texto(texto):\n",
    "  '''(str) -> str\n",
    "  Essa funcao recebe uma string, deixa tudo em minusculo, filtra apenas letras,\n",
    "  retira stopwords, lemmatiza e retorna a string resultante.\n",
    "  '''\n",
    "  texto = texto.lower()\n",
    "\n",
    "  texto = re.sub(r\"[\\W\\d_]+\", \" \", texto)\n",
    "\n",
    "  texto = [pal for pal in texto.split() if pal not in stopwords_en]\n",
    "\n",
    "  spc_texto = spc_en(\" \".join(texto))\n",
    "  tokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in spc_texto]\n",
    "  \n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_alexa = pd.read_csv('df_preprocessed.csv')\n",
    "    df_alexa.dropna(inplace=True)\n",
    "    \n",
    "except:\n",
    "    print('Dataset preprocessado nao encontrado, criando novo dataset...')\n",
    "    try:\n",
    "        df = pd.read_csv('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')\n",
    "\n",
    "        df_alexa = df.drop(['id','dateAdded', 'dateUpdated', 'asins', 'keys', 'manufacturer', 'manufacturerNumber', \n",
    "        'reviews.date', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id',\n",
    "         'reviews.numHelpful', 'reviews.sourceURLs', 'reviews.username', 'imageURLs', 'primaryCategories', 'categories',\n",
    "         'brand', 'name', 'sourceURLs'], axis=1)\n",
    "\n",
    "        df_alexa.fillna('', inplace = True) # para nao ter problemas com nulos na concatenacao\n",
    "\n",
    "        # concatenando as duas colunas\n",
    "        df_alexa['verified_reviews'] = df_alexa['reviews.text'] + ' ' + df_alexa['reviews.title']\n",
    "        # removendo entradas sem texto\n",
    "        df_alexa = df_alexa[df_alexa['verified_reviews'] != ' ']\n",
    "\n",
    "\n",
    "        # transformando rating em feedback 0 e 1\n",
    "        labels = []\n",
    "        for score in df_alexa['reviews.rating']:\n",
    "            if score > 3:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "        df_alexa['feedback'] = labels\n",
    "        # Aplica a funcao nas reviews do dataset\n",
    "        df_alexa['verified_reviews'] = df_alexa['verified_reviews'].apply(limpa_texto)\n",
    "\n",
    "        # Salva o dataset preprocessado\n",
    "        \n",
    "        df_alexa.to_csv('df_preprocessed.csv', index=False)\n",
    "    except:\n",
    "        print('Dataset original nao encontrado')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25543\n",
       "0     2786\n",
       "Name: feedback, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alexa['feedback'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = df_alexa['verified_reviews']\n",
    "# Importando o TfidfVectorizer\n",
    "\n",
    "# Instanciando o TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# Vetorizando\n",
    "X_tfidf = tfidf_vect.fit_transform(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf.toarray(), df_alexa['feedback'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    classifier = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    classifier.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # evaluate loaded model on test data\n",
    "    classifier.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "except:\n",
    "    # create model and save\n",
    "    classifier = tf.keras.models.Sequential()\n",
    "    classifier.add(tf.keras.layers.Dense(units = 10, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    classifier.add(tf.keras.layers.Dense(units = 10, activation='relu'))\n",
    "    classifier.add(tf.keras.layers.Dense(units = 1, activation='sigmoid'))\n",
    "\n",
    "    classifier.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    epochs_hist = classifier.fit(X_train, y_train, epochs=100, batch_size=50,  verbose=2, validation_split=0.2)\n",
    "\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = classifier.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    classifier.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90       543\n",
      "           1       0.99      0.99      0.99      5123\n",
      "\n",
      "    accuracy                           0.98      5666\n",
      "   macro avg       0.94      0.95      0.94      5666\n",
      "weighted avg       0.98      0.98      0.98      5666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification_report\n",
    "y_pred_test = (classifier.predict(X_test) > 0.5).astype(int)\n",
    "print(classification_report(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 997us/step\n",
      "Positivos: 78 | Negativos: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if len(coments_translate) == 0:\n",
    "    print('Nenhum comentário para ser classificado')\n",
    "else:\n",
    "    c = tfidf_vect.transform(coments_translate).toarray()\n",
    "    predict = (classifier.predict(c) > 0.5).astype(int)\n",
    "\n",
    "    cont_pos = 0\n",
    "    cont_neg = 0\n",
    "    for p in predict:\n",
    "        if p == 1:\n",
    "            cont_pos += 1\n",
    "        else:\n",
    "            cont_neg += 1\n",
    "\n",
    "    print(f'Positivos: {cont_pos} | Negativos: {cont_neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
